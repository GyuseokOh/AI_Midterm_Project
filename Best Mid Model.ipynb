{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"인공지능 과제 제일 높은거.ipynb","provenance":[{"file_id":"17JxkhmKkDcLo26jUuaQ8AQui2An-E6TU","timestamp":1621267086875}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1ekgDsG3f1t","executionInfo":{"status":"ok","timestamp":1621836029610,"user_tz":-540,"elapsed":24399,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"6d753bb3-b0a9-45db-f558-8025ee97e46a"},"source":["#https://github.com/jongbinryuclass/ajou_2021_spring_ai\n","\n","import numpy as np\n","from google.colab import drive\n","\n","#load train data and label\n","drive.mount('/content/drive')\n","default_path = \"/content/drive/MyDrive/인공지능 수업/데이터/\"\n","\n","# v1\n","v1_train_data_path = default_path + \"v1_train_data.npy\"\n","v1_train_label_path = default_path + \"v1_train_label.npy\"\n","v1_test_gallery_data_path = default_path + \"v1_test_gallery_data.npy\"\n","v1_test_gallery_label_path = default_path + \"v1_test_gallery_label.npy\"\n","v1_test_query_data_path = default_path + \"v1_test_query_data.npy\"\n","\n","# v2\n","v2_test_gallery_data_path = default_path + \"v2_test_gallery_data.npy\"\n","v2_test_gallery_label_path = default_path + \"v2_test_gallery_label.npy\"\n","v2_test_query_data_path = default_path + \"v2_test_query_data.npy\"\n","v2_test_query_label_path = default_path + \"v2_test_query_label.npy\"\n","\n","\n","\n","\n","#인공지능 학습 데이터\n","train_data = np.load(v1_train_data_path, allow_pickle=True)\n","train_label = np.load(v1_train_label_path, allow_pickle=True)  \n","\n","#query는 테스트를 위한 이미지, 어떤 이미지인가?\n","#gallary는 데이터베이스에 저장되어 있는 사진, query 하나하나마다 비교해서 비슷한걸 리턴\n","test_query_label = np.load(v2_test_query_label_path, allow_pickle=True)\n","test_gallery_label = np.load(v2_test_gallery_label_path, allow_pickle=True)  \n","\n","v1_test_query_data = np.load(v1_test_query_data_path, allow_pickle=True)\n","test_query_data = np.load(v2_test_query_data_path, allow_pickle=True)\n","test_gallery_data = np.load(v2_test_gallery_data_path, allow_pickle=True)\n","\n","num_train_data = train_data.shape[0]"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tei9ATIsqpkb","executionInfo":{"status":"ok","timestamp":1621836029999,"user_tz":-540,"elapsed":391,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"393f2313-9a89-46f3-c027-32d21e4edc88"},"source":["label_reducer = -1\n","\n","#인공지능 학습 데이터\n","train_data = np.load(v1_train_data_path, allow_pickle=True)\n","train_label = np.load(v1_train_label_path, allow_pickle=True)+label_reducer\n","\n","# test_gallery_data\n","v1_test_gallery_data = np.load(v1_test_gallery_data_path, allow_pickle=True)\n","v2_test_gallery_data = np.load(v2_test_gallery_data_path, allow_pickle=True)\n","# print(\"test_gallery_data\", (np.sort(v1_test_gallery_data) == np.sort(v2_test_gallery_data)).all())\n","\n","\n","# test_gallery_label\n","v1_test_gallery_label = np.load(v1_test_gallery_label_path, allow_pickle=True)+label_reducer\n","v2_test_gallery_label = np.load(v2_test_gallery_label_path, allow_pickle=True)+label_reducer\n","# print(\"test_gallery_label\", (v1_test_gallery_label == v2_test_gallery_label).all())\n","\n","# test_query_data\n","v1_test_query_data = np.load(v1_test_query_data_path, allow_pickle=True)\n","v2_test_query_data = np.load(v2_test_query_data_path, allow_pickle=True)\n","v2_test_query_label = np.load(v2_test_query_label_path, allow_pickle=True)+label_reducer\n","# print(\"test_query_data\", v1_test_query_data == v2_test_query_data)\n","\n","# v1_test_gallery_label = v1_test_gallery_label - 1\n","# v2_test_gallery_label = v2_test_gallery_label - 1\n","\n","train_data = np.concatenate((train_data, v1_test_gallery_data, v2_test_gallery_data, v2_test_query_data))\n","train_label = np.concatenate((train_label, v1_test_gallery_label, v2_test_gallery_label, v2_test_query_label))\n","\n","print(np.min(v1_test_gallery_label), np.max(v1_test_gallery_label))\n","print(np.min(v2_test_gallery_label), np.max(v2_test_gallery_label))\n","print(train_data.shape)\n","print(train_label.shape)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["0 199\n","0 199\n","(2700, 1, 512, 1, 1)\n","(2700,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAG-bN2DvXoz","executionInfo":{"status":"ok","timestamp":1621836029999,"user_tz":-540,"elapsed":10,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"fd68945e-8227-485d-eb43-c1dbdd33970b"},"source":["print(np.min(train_data), np.max(train_data))\n","print(np.min(train_label), np.max(train_label))\n","# print(np.min(v1_test_gallery_label), np.max(v1_test_gallery_label))\n","# print(np.min(v2_test_gallery_label), np.max(v2_test_gallery_label))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["0.55994165 2.0335684\n","0 199\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zi_CRigwB1Ed","executionInfo":{"status":"ok","timestamp":1621836030000,"user_tz":-540,"elapsed":9,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"6a74dd1d-562f-425b-b92d-d57c016a131d"},"source":["print(train_data.shape) # train_data is a list\n","print(num_train_data) # the number of train_data\n","print(train_label.shape)\n","print(test_query_data.shape)\n","print(test_gallery_data.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(2700, 1, 512, 1, 1)\n","2000\n","(2700,)\n","(500, 1, 512, 1, 1)\n","(100, 1, 512, 1, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q0PR9AGXJjCF","executionInfo":{"status":"ok","timestamp":1621836030001,"user_tz":-540,"elapsed":8,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#쿼리 하나하나마다 갤러리에 어떤거랑 가장 가까운지 비교해 반환\n","def getNearestNeibor(query, gallery):\n","  num_query = query.shape[0]\n","  num_gallery = gallery.shape[0]\n","  nn_idx = np.zeros(num_query)\n","  #쿼리를 도는것\n","  for q in range(num_query):\n","    dist = np.zeros(num_gallery)\n","    #갤러리를 도는 것\n","    for g in range(num_gallery):\n","      #벡터와 벡터 사이의 차\n","      #의 제곱\n","      #dist의 길이는 gallary만큼의 길이를 가짐 why? 한 쿼리에 대해 모든 갤러리 샘플 비교해 디스턴스 구하기 떄문\n","      dist[g] = np.sqrt(np.sum((query[q,:] - gallery[g,:]) ** 2, axis=0))\n","    #q번째 쿼리 샘플에 대해 가장 가까운 갤러리 인덱스 저장\n","    nn_idx[q] = np.argmin(dist)\n","  return nn_idx"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Dm87jArdHVh","executionInfo":{"status":"ok","timestamp":1621836030001,"user_tz":-540,"elapsed":7,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"7c120f80-d023-4cca-9f7a-df1d8e45e837"},"source":["import numpy as np\n","a=[1,2,3,4,5]\n","b=[1,3,5,7,9]\n","\n","c=np.array(a)-np.array(b)\n","print(c)\n","print(c**2)\n","print(np.sum(c**2))\n","#벡터간의 거리\n","print(np.sqrt(np.sum(c**2)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[ 0 -1 -2 -3 -4]\n","[ 0  1  4  9 16]\n","30\n","5.477225575051661\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K9fzmh_pr3ZW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621836030562,"user_tz":-540,"elapsed":566,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"1ccb7a22-cc33-4196-8780-c18a5c6bb4ed"},"source":["#스퀴즈 함수는 1을 없애주는 함수\n","nn_idx = getNearestNeibor(test_query_data.squeeze(), test_gallery_data.squeeze())\n","print(test_query_data.shape)\n","print(test_query_data.squeeze().shape)\n","print(nn_idx)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(500, 1, 512, 1, 1)\n","(500, 512)\n","[35. 55. 42. 76. 40. 54. 42. 76. 96. 68. 55. 67. 78.  0. 42. 25. 34. 89.\n"," 38. 42. 78. 33. 35.  6. 89. 96. 96. 16. 96. 78. 71. 65. 41.  6. 38.  7.\n"," 24. 92. 96. 13. 64. 41. 88. 96. 96. 95. 25. 77. 16. 21. 76.  0. 70.  0.\n"," 93. 34. 67. 68. 98. 24. 34. 93. 31. 92. 68. 76. 34. 76. 93. 45. 38. 78.\n"," 42. 77. 18. 34. 76. 78. 76. 24. 76. 76. 31. 76. 42. 78. 42. 89. 34. 77.\n"," 24. 30. 67. 93. 25. 78. 94. 31. 24. 87. 24. 34. 48.  7. 99. 38. 42. 64.\n"," 98. 54. 76. 90. 34. 91. 13. 38.  6. 61. 71. 76. 35. 53. 74. 68. 25. 24.\n"," 96.  3. 42. 45. 96.  0. 53. 92. 76. 38.  6. 76. 25. 35. 23. 77. 98. 40.\n","  6. 38. 81. 91. 55. 45.  5. 53. 53. 81. 93. 96. 46. 13. 42. 55. 55. 76.\n"," 76. 19. 24. 38. 25. 46. 22. 34. 89. 78. 92. 16.  1. 79. 76. 96. 76. 53.\n"," 55. 24. 55. 41. 21. 67. 83. 42. 31. 19.  6. 65. 45. 88. 40. 64. 64. 25.\n"," 54. 77. 53.  9. 24. 83. 45. 76. 78. 25. 31. 77. 76. 78. 42. 87. 42. 76.\n"," 42. 42. 19. 76. 93. 92. 34. 68. 76. 91. 77. 30. 92. 96. 92. 55. 34. 34.\n"," 31. 74. 78. 72. 34. 72. 91. 67. 77.  7. 78. 79. 24. 93. 54.  9. 25. 90.\n"," 93. 68. 95. 76. 11. 96. 48. 11. 48. 76. 89. 38. 74. 78. 93. 92. 76. 96.\n"," 10. 23. 98. 91. 40. 31. 38. 19. 78. 42. 82. 47. 93. 34. 23.  6. 79. 77.\n"," 61. 34. 27. 76. 21. 41. 48. 96. 34. 77. 76. 48. 68. 25.  6. 24.  3. 55.\n"," 78. 96. 76. 31. 76. 11. 68. 96. 41. 78. 76. 46. 35. 31. 24. 60. 77. 24.\n"," 76. 55. 42. 76. 95. 55. 76. 76. 76. 76. 14. 55. 21. 93. 21. 76. 24. 74.\n"," 88. 71. 76. 11. 93. 55. 96. 42. 25. 78. 81. 76. 16. 89.  9. 45. 34.  6.\n"," 41. 55. 19. 31. 90. 76. 92. 93. 47. 76. 34. 55. 47. 61.  9. 31. 19. 16.\n"," 24. 76. 72. 40. 48. 24. 27. 24.  6. 76. 92.  4. 22. 76. 34. 24. 38.  7.\n"," 53. 31. 15. 78. 92. 78. 89. 42. 24. 92. 76. 35. 30. 24. 60. 42. 34.  0.\n","  5.  0. 78. 34. 34. 42. 42. 31. 55.  5. 31. 21. 88. 46. 34. 93. 28. 11.\n"," 45. 16. 55. 16. 16.  9. 19. 24. 24. 81. 42. 90. 24. 34. 76. 95. 88. 53.\n"," 46. 93. 93. 34. 76. 21. 24. 11. 31.  6. 93. 91. 76. 88. 74. 42. 42. 55.\n","  6. 76. 33. 24. 96. 76. 76. 85. 60. 24. 76. 76. 92. 95. 45.  6. 78. 25.\n"," 76. 76. 55. 36. 24. 41. 48. 25.  4. 76. 74. 12. 92. 45.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vFVVC2HsG5b","executionInfo":{"status":"ok","timestamp":1621836031189,"user_tz":-540,"elapsed":633,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"c2a47fb7-8eb7-4e56-c0ac-ca704792789c"},"source":["nn_idx = getNearestNeibor(test_query_data.squeeze(), test_gallery_data.squeeze())\n","print(test_query_data.shape)\n","#nn.idx는 쿼리 데이터에 대한 가장 가까운 갤러리 데이터의 인덱스를 구한 것\n","#따라서 nn.idx에 저장된 query를 가지고 예측한 gallary의 index값과 실제 query 데이터의 인텍스 값과 비교해 맞은 것의 개수를 구해야함\n","#따라서 이렇게 맞은 개수를 구함\n","print(np.sum(test_query_label == test_gallery_label[np.int64(nn_idx)]))\n","#nn_idx_rand = np.random.randint(1,100,1000)\n","#print(np.sum(tet_query_label == test_gallery_label[np.int64(nn_idx_rand)]))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(500, 1, 512, 1, 1)\n","12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pN5jVQPmgfTH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621836031189,"user_tz":-540,"elapsed":8,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"4246cae1-01e5-4628-dc12-9af0a9f9d671"},"source":["x=test_query_label == test_gallery_label[np.int64(nn_idx)]\n","print(x)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False  True False False False False False False False False\n"," False False False False False False False False False False  True False\n"," False False False False False False False False False  True False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False  True False False False False  True False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False  True False False False False False False False False False\n"," False False False False False False False False False False False  True\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False  True False False False False False False False\n"," False False False False False False False False False False False  True\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True False False False\n"," False False False False False False False False]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06ihfx-t453o","executionInfo":{"status":"ok","timestamp":1621836031190,"user_tz":-540,"elapsed":6,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"30d4a19e-525a-4249-d3b5-19c5e9484add"},"source":["#정확도는 ?/1000이 됨\n","pred_test_gallery_label_txt = list_data = [str(int(a)).strip('\\n\\r') for a in test_gallery_label[np.int64(nn_idx)]]\n","print(pred_test_gallery_label_txt)\n","#print(train_data[0:10])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["['157', '16', '168', '68', '13', '58', '168', '68', '108', '54', '16', '72', '35', '86', '168', '139', '22', '114', '200', '168', '35', '148', '157', '116', '114', '108', '108', '59', '108', '35', '163', '188', '180', '116', '200', '37', '132', '73', '108', '55', '48', '180', '143', '108', '108', '130', '139', '36', '59', '75', '68', '86', '144', '86', '182', '22', '72', '54', '66', '132', '22', '182', '106', '73', '54', '68', '22', '68', '182', '47', '200', '35', '168', '36', '175', '22', '68', '35', '68', '132', '68', '68', '106', '68', '168', '35', '168', '114', '22', '36', '132', '60', '72', '182', '139', '35', '38', '106', '132', '111', '132', '22', '23', '37', '30', '200', '168', '48', '66', '58', '68', '110', '22', '71', '55', '200', '116', '81', '163', '68', '157', '125', '32', '54', '139', '132', '108', '193', '168', '47', '108', '86', '125', '73', '68', '200', '116', '68', '139', '157', '172', '36', '66', '13', '116', '200', '118', '71', '16', '47', '170', '125', '125', '118', '182', '108', '18', '55', '168', '16', '16', '68', '68', '51', '132', '200', '139', '18', '29', '22', '114', '35', '73', '59', '83', '122', '68', '108', '68', '125', '16', '132', '16', '180', '75', '72', '198', '168', '106', '51', '116', '188', '47', '143', '13', '48', '48', '139', '58', '36', '125', '151', '132', '198', '47', '68', '35', '139', '106', '36', '68', '35', '168', '111', '168', '68', '168', '168', '51', '68', '182', '73', '22', '54', '68', '71', '36', '60', '73', '108', '73', '16', '22', '22', '106', '32', '35', '28', '22', '28', '71', '72', '36', '37', '35', '122', '132', '182', '58', '151', '139', '110', '182', '54', '130', '68', '90', '108', '23', '90', '23', '68', '114', '200', '32', '35', '182', '73', '68', '108', '100', '172', '66', '71', '13', '106', '200', '51', '35', '168', '120', '27', '182', '22', '172', '116', '122', '36', '81', '22', '107', '68', '75', '180', '23', '108', '22', '36', '68', '23', '54', '139', '116', '132', '193', '16', '35', '108', '68', '106', '68', '90', '54', '108', '180', '35', '68', '18', '157', '106', '132', '76', '36', '132', '68', '16', '168', '68', '130', '16', '68', '68', '68', '68', '77', '16', '75', '182', '75', '68', '132', '32', '143', '163', '68', '90', '182', '16', '108', '168', '139', '35', '118', '68', '59', '114', '151', '47', '22', '116', '180', '16', '51', '106', '110', '68', '73', '182', '27', '68', '22', '16', '27', '81', '151', '106', '51', '59', '132', '68', '28', '13', '23', '132', '107', '132', '116', '68', '73', '179', '29', '68', '22', '132', '200', '37', '125', '106', '19', '35', '73', '35', '114', '168', '132', '73', '68', '157', '60', '132', '76', '168', '22', '86', '170', '86', '35', '22', '22', '168', '168', '106', '16', '170', '106', '75', '143', '18', '22', '182', '64', '90', '47', '59', '16', '59', '59', '151', '51', '132', '132', '118', '168', '110', '132', '22', '68', '130', '143', '125', '18', '182', '182', '22', '68', '75', '132', '90', '106', '116', '182', '71', '68', '143', '32', '168', '168', '16', '116', '68', '148', '132', '108', '68', '68', '10', '76', '132', '68', '68', '73', '130', '47', '116', '35', '139', '68', '68', '16', '62', '132', '180', '23', '139', '179', '68', '32', '146', '73', '47']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"brGnT7Sx74D2","executionInfo":{"status":"ok","timestamp":1621836034240,"user_tz":-540,"elapsed":3054,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#Import Libraries\n","\n","\n","from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOKs4Y-t77hf","executionInfo":{"status":"ok","timestamp":1621836034240,"user_tz":-540,"elapsed":14,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["class MyDataset(torch.utils.data.Dataset):\n","  #torch.utils.data.Dataset를 상속받아 바꿔줌\n","  #우리만의 새를 위한 데이터셋 만들어줌\n","  def __init__(self, data, label=[]):\n","    #Dataset 새로 정의함\n","    self.data = data\n","    self.label = label\n","    \n","  def __len__(self):\n","    return self.data.shape[0]\n","    #데이터 길이 줌\n","\n","  def __getitem__(self, idx):\n","    if self.label == []: #test phase\n","      return self.data[idx, :, :, :]\n","    else: #train phase\n","      return self.data[idx, :, :, :], self.label[idx]\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"0c-ReVjBLmcw","executionInfo":{"status":"ok","timestamp":1621836034241,"user_tz":-540,"elapsed":14,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["# train_data = train_data.reshape(2000, 32, 16)\n","# print(train_data.shape)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"RULHJRbOH89h","executionInfo":{"status":"ok","timestamp":1621836034241,"user_tz":-540,"elapsed":13,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"d0e40c4b-8a66-46cf-94b7-755cadc2ad72"},"source":["#지금까지 제일 잘나온거\n","\"\"\"\n","args={}\n","kwargs={}\n","#하이퍼파라미터 설정\n","#인공지능이 학습하는게 아니라 사람이 사전에 결정해야 하는 것\n","#배치 사이즈, 테스트 배치 사이즈 등\n","#한번에 몇개의 데이터를 가져올 것인지\n","args['batch_size']=64\n","args['test_batch_size']=16\n","#에폭은 트레인 데이터 전체를 몇번 학습시킬 것인지, 50이면 트레인 데이터 전체를 50번 학습시킴\n","args['epochs']=100  #The number of Epochs is the number of times you go through the full dataset. \n","#배치가 커지면 러닝 레이트가 커져야됨\n","#배치가 작아지면 러닝 레이트도 작아져야됨\n","#왜? 배치가 커지면 많이 학습해야 하므로 러닝 레이트 커져야됨\n","#반대의 경우에도 마찬가지\n","#args['lr']=0.005 #Learning rate is how fast it will decend. \n","args['lr']=0.0005 #Learning rate is how fast it will decend. \n","#관성을 얼마나 유지할 것인지, 이전 학습 결과를 얼마나 크게 반영할 것인가\n","args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n","\n","args['seed']=1 #random seed\n","#프린트를 얼마나 자주 할것인지, 값이 작으면 세밀하게 출력함\n","args['log_interval']=10\n","args['cuda']=False\n","\n","\n","\n","class Net(nn.Module):\n","    #This defines the structure of the NN.\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(512, 512, kernel_size=1)\n","        self.bn1 = nn.BatchNorm2d(512)\n","        self.conv2 = nn.Conv2d(512, 200, kernel_size=1)\n","        #self.conv2_drop = nn.Dropout2d()  #Dropout\n","        #self.fc1 = nn.Linear(512, 64)\n","        self.fc2 = nn.Linear(512, 200)\n","\n","    def forward(self, x):\n","        #Convolutional Layer/Pooling Layer/Activation\n","        #액티베이션 함수\n","        #print(x.shape)\n","        x = F.relu( self.bn1(self.conv1(x)), 2)\n","        #Convolutional Layer/Dropout/Pooling Layer/Activation\n","        #여기서 1.1.512를 1*512로 차원을 바꿔줌\n","        #print(x.shape)\n","        x = x.view(-1, 512)\n","        #print(x.shape)\n","        #Fully Connected Layer/Activation\n","        #x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        #Fully Connected Layer/Activation\n","        x = self.fc2(x)\n","        #Softmax gets probabilities. \n","        #softmax로 정규화함\n","        return F.log_softmax(x, dim=1)\n","\n","\n","\n","\n","def train(epoch):\n","    model.train()\n","    #한번에 배치 사이즈만큼 데이터 가져옴\n","    #타겟은 정답 레이블\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if args['cuda']:\n","            data, target = data.cuda(), target.cuda()\n","        #Variables in Pytorch are differenciable. \n","        data, target = Variable(data), Variable(target)\n","        #This will zero out the gradients for this batch. \n","        optimizer.zero_grad()\n","        #뉴럴넷을 객체화한 모델에 넣음\n","        #그러면 포워드 함수가 들어감\n","        #포워드 함수의 엑스에 data가 들어감\n","        output = model(data)\n","        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n","        #아웃풋과 타겟을 비교해 로스를 구함\n","        #loss = F.nll_loss(output, target)\n","        criterion = nn.CrossEntropyLoss()\n","        loss = criterion(output, target)\n","        #dloss/dx for every Variable \n","        loss.backward()\n","        #print(loss.data)\n","        #to do a one-step update on our parameter.\n","        #파라미터 업데이트\n","        optimizer.step()\n","        #Print out the loss periodically. \n","        #성능 출력\n","        if batch_idx % args['log_interval'] == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.data))\n","\n","def test():\n","    #여기서는 백프로파게이션과 옵티마이저 펑션 없음\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        if args['cuda']:\n","            data, target = data.cuda(), target.cuda()\n","        #test label은 학습에 포함이 되면 안됨!!!!\n","        data, target = Variable(data, volatile=True), Variable(target)\n","        output = model(data)\n","        criterion = nn.CrossEntropyLoss()\n","        test_loss += criterion(output, target).data # sum up batch loss\n","        #test_loss += F.nll_loss(output, target, size_average=False).data # sum up batch loss\n","        #프레딕션 한게 얼마만큼 정답에 가까운지 계산\n","        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n","        #정답의 갯수 세서 correct에 넣음\n","        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\"\"\""],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nargs={}\\nkwargs={}\\n#하이퍼파라미터 설정\\n#인공지능이 학습하는게 아니라 사람이 사전에 결정해야 하는 것\\n#배치 사이즈, 테스트 배치 사이즈 등\\n#한번에 몇개의 데이터를 가져올 것인지\\nargs['batch_size']=64\\nargs['test_batch_size']=16\\n#에폭은 트레인 데이터 전체를 몇번 학습시킬 것인지, 50이면 트레인 데이터 전체를 50번 학습시킴\\nargs['epochs']=100  #The number of Epochs is the number of times you go through the full dataset. \\n#배치가 커지면 러닝 레이트가 커져야됨\\n#배치가 작아지면 러닝 레이트도 작아져야됨\\n#왜? 배치가 커지면 많이 학습해야 하므로 러닝 레이트 커져야됨\\n#반대의 경우에도 마찬가지\\n#args['lr']=0.005 #Learning rate is how fast it will decend. \\nargs['lr']=0.0005 #Learning rate is how fast it will decend. \\n#관성을 얼마나 유지할 것인지, 이전 학습 결과를 얼마나 크게 반영할 것인가\\nargs['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\\n\\nargs['seed']=1 #random seed\\n#프린트를 얼마나 자주 할것인지, 값이 작으면 세밀하게 출력함\\nargs['log_interval']=10\\nargs['cuda']=False\\n\\n\\n\\nclass Net(nn.Module):\\n    #This defines the structure of the NN.\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.conv1 = nn.Conv2d(512, 512, kernel_size=1)\\n        self.bn1 = nn.BatchNorm2d(512)\\n        self.conv2 = nn.Conv2d(512, 200, kernel_size=1)\\n        #self.conv2_drop = nn.Dropout2d()  #Dropout\\n        #self.fc1 = nn.Linear(512, 64)\\n        self.fc2 = nn.Linear(512, 200)\\n\\n    def forward(self, x):\\n        #Convolutional Layer/Pooling Layer/Activation\\n        #액티베이션 함수\\n        #print(x.shape)\\n        x = F.relu( self.bn1(self.conv1(x)), 2)\\n        #Convolutional Layer/Dropout/Pooling Layer/Activation\\n        #여기서 1.1.512를 1*512로 차원을 바꿔줌\\n        #print(x.shape)\\n        x = x.view(-1, 512)\\n        #print(x.shape)\\n        #Fully Connected Layer/Activation\\n        #x = F.relu(self.fc1(x))\\n        x = F.dropout(x, training=self.training)\\n        #Fully Connected Layer/Activation\\n        x = self.fc2(x)\\n        #Softmax gets probabilities. \\n        #softmax로 정규화함\\n        return F.log_softmax(x, dim=1)\\n\\n\\n\\n\\ndef train(epoch):\\n    model.train()\\n    #한번에 배치 사이즈만큼 데이터 가져옴\\n    #타겟은 정답 레이블\\n    for batch_idx, (data, target) in enumerate(train_loader):\\n        if args['cuda']:\\n            data, target = data.cuda(), target.cuda()\\n        #Variables in Pytorch are differenciable. \\n        data, target = Variable(data), Variable(target)\\n        #This will zero out the gradients for this batch. \\n        optimizer.zero_grad()\\n        #뉴럴넷을 객체화한 모델에 넣음\\n        #그러면 포워드 함수가 들어감\\n        #포워드 함수의 엑스에 data가 들어감\\n        output = model(data)\\n        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\\n        #아웃풋과 타겟을 비교해 로스를 구함\\n        #loss = F.nll_loss(output, target)\\n        criterion = nn.CrossEntropyLoss()\\n        loss = criterion(output, target)\\n        #dloss/dx for every Variable \\n        loss.backward()\\n        #print(loss.data)\\n        #to do a one-step update on our parameter.\\n        #파라미터 업데이트\\n        optimizer.step()\\n        #Print out the loss periodically. \\n        #성능 출력\\n        if batch_idx % args['log_interval'] == 0:\\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\\n                epoch, batch_idx * len(data), len(train_loader.dataset),\\n                100. * batch_idx / len(train_loader), loss.data))\\n\\ndef test():\\n    #여기서는 백프로파게이션과 옵티마이저 펑션 없음\\n    model.eval()\\n    test_loss = 0\\n    correct = 0\\n    for data, target in test_loader:\\n        if args['cuda']:\\n            data, target = data.cuda(), target.cuda()\\n        #test label은 학습에 포함이 되면 안됨!!!!\\n        data, target = Variable(data, volatile=True), Variable(target)\\n        output = model(data)\\n        criterion = nn.CrossEntropyLoss()\\n        test_loss += criterion(output, target).data # sum up batch loss\\n        #test_loss += F.nll_loss(output, target, size_average=False).data # sum up batch loss\\n        #프레딕션 한게 얼마만큼 정답에 가까운지 계산\\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\\n        #정답의 갯수 세서 correct에 넣음\\n        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\\n\\n    test_loss /= len(test_loader.dataset)\\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\\n        test_loss, correct, len(test_loader.dataset),\\n        100. * correct / len(test_loader.dataset)))\\n\""]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"sOjuh-LM6ZQ8","executionInfo":{"status":"ok","timestamp":1621836034613,"user_tz":-540,"elapsed":377,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#지금까지 제일 잘나온거\n","\n","args={}\n","kwargs={}\n","#하이퍼파라미터 설정\n","#인공지능이 학습하는게 아니라 사람이 사전에 결정해야 하는 것\n","#배치 사이즈, 테스트 배치 사이즈 등\n","#한번에 몇개의 데이터를 가져올 것인지\n","args['batch_size']=64\n","args['test_batch_size']=16\n","#에폭은 트레인 데이터 전체를 몇번 학습시킬 것인지, 50이면 트레인 데이터 전체를 50번 학습시킴\n","args['epochs']=100  #The number of Epochs is the number of times you go through the full dataset. \n","#배치가 커지면 러닝 레이트가 커져야됨\n","#배치가 작아지면 러닝 레이트도 작아져야됨\n","#왜? 배치가 커지면 많이 학습해야 하므로 러닝 레이트 커져야됨\n","#반대의 경우에도 마찬가지\n","#args['lr']=0.005 #Learning rate is how fast it will decend. \n","args['lr']=0.0005 #Learning rate is how fast it will decend. \n","#관성을 얼마나 유지할 것인지, 이전 학습 결과를 얼마나 크게 반영할 것인가\n","args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n","\n","args['seed']=1 #random seed\n","#프린트를 얼마나 자주 할것인지, 값이 작으면 세밀하게 출력함\n","args['log_interval']=10\n","args['cuda']=False\n","\n","\n","\n","class Net(nn.Module):\n","    #This defines the structure of the NN.\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(512, 512, kernel_size=1)\n","        self.bn1 = nn.BatchNorm2d(512)\n","        self.conv2 = nn.Conv2d(512, 200, kernel_size=1)\n","        #self.conv2_drop = nn.Dropout2d()  #Dropout\n","        #self.fc1 = nn.Linear(512, 64)\n","        self.fc2 = nn.Linear(512, 200)\n","\n","    def forward(self, x):\n","        #Convolutional Layer/Pooling Layer/Activation\n","        #액티베이션 함수\n","        #print(x.shape)\n","        x = F.relu( self.bn1(self.conv1(x)), 2)\n","        #Convolutional Layer/Dropout/Pooling Layer/Activation\n","        #여기서 1.1.512를 1*512로 차원을 바꿔줌\n","        #print(x.shape)\n","        x = x.view(-1, 512)\n","        #print(x.shape)\n","        #Fully Connected Layer/Activation\n","        #x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        #Fully Connected Layer/Activation\n","        x = self.fc2(x)\n","        #Softmax gets probabilities. \n","        #softmax로 정규화함\n","        return F.log_softmax(x, dim=1)\n","\n","\n","\n","\n","def train(epoch):\n","    model.train()\n","    #한번에 배치 사이즈만큼 데이터 가져옴\n","    #타겟은 정답 레이블\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if args['cuda']:\n","            data, target = data.cuda(), target.cuda()\n","        #Variables in Pytorch are differenciable. \n","        data, target = Variable(data), Variable(target)\n","        #This will zero out the gradients for this batch. \n","        optimizer.zero_grad()\n","        #뉴럴넷을 객체화한 모델에 넣음\n","        #그러면 포워드 함수가 들어감\n","        #포워드 함수의 엑스에 data가 들어감\n","        output = model(data)\n","        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n","        #아웃풋과 타겟을 비교해 로스를 구함\n","        #loss = F.nll_loss(output, target)\n","        criterion = nn.CrossEntropyLoss()\n","        loss = criterion(output, target)\n","        #dloss/dx for every Variable \n","        loss.backward()\n","        #print(loss.data)\n","        #to do a one-step update on our parameter.\n","        #파라미터 업데이트\n","        optimizer.step()\n","        #Print out the loss periodically. \n","        #성능 출력\n","        if batch_idx % args['log_interval'] == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.data))\n","\n","def test():\n","    #여기서는 백프로파게이션과 옵티마이저 펑션 없음\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        if args['cuda']:\n","            data, target = data.cuda(), target.cuda()\n","        #test label은 학습에 포함이 되면 안됨!!!!\n","        data, target = Variable(data, volatile=True), Variable(target)\n","        output = model(data)\n","        criterion = nn.CrossEntropyLoss()\n","        test_loss += criterion(output, target).data # sum up batch loss\n","        #test_loss += F.nll_loss(output, target, size_average=False).data # sum up batch loss\n","        #프레딕션 한게 얼마만큼 정답에 가까운지 계산\n","        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n","        #정답의 갯수 세서 correct에 넣음\n","        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1yUb3qlHBM0","executionInfo":{"status":"ok","timestamp":1621836034615,"user_tz":-540,"elapsed":5,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["# train_data_reshaped = np.reshape(train_data,(train_data.shape[0], train_data.shape[2], train_data.shape[3], train_data.shape[4]))\n","# train_label = np.squeeze(train_label)\n","# train_loader = torch.utils.data.DataLoader(\n","#     #내 데이터를 객체화\n","#     #마이데이터셋 클래스에 인풋으로 들어감\n","#     #데이터에 추가적인 기능들을 더하는 것이 데이터로더\n","#     #트레인테스트는 그냥 데이터다\n","#     MyDataset(torch.from_numpy(train_data_reshaped), torch.squeeze(torch.from_numpy(train_label))),\n","#     batch_size=args['batch_size'], shuffle=True, **kwargs)\n","# test_loader = torch.utils.data.DataLoader(\n","#     MyDataset(torch.from_numpy(train_data_reshaped), torch.squeeze(torch.from_numpy(train_label))),\n","#     batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n","\n","# train_data_reshaped1 = np.reshape(train_data[ :-1:2,:,:,:,:],(train_data.shape[0]//2, train_data.shape[2], train_data.shape[3], train_data.shape[4]))\n","# train_label = np.squeeze(train_label)\n","\n","\n","# print(train_label[:-1:2].shape)\n","# print(train_label[1:-1:2].shape)\n","# train_loader = torch.utils.data.DataLoader(\n","#     MyDataset(torch.from_numpy(train_data_reshaped1), torch.squeeze(torch.from_numpy(train_label[:-1:2]))),\n","#     batch_size=args['batch_size'], shuffle=True, **kwargs)\n","# train_data_reshaped2 = np.reshape(train_data[1:-1:2,:,:,:,:],(train_data.shape[0]//2-1, train_data.shape[2], train_data.shape[3], train_data.shape[4]))\n","# test_loader = torch.utils.data.DataLoader(\n","#     MyDataset(torch.from_numpy(train_data_reshaped2), torch.squeeze(torch.from_numpy(train_label[1:-1:2]))),\n","#     batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n","\n","##changwan edit\n","\n","# from sklearn.model_selection import train_test_split\n","\n","# x_train, x_val, y_train, y_val = train_test_split(\n","#     train_data.reshape(2700, 512, 1, 1), train_label, test_size = 0.2, random_state = 1)\n","\n","# print(x_train.shape)\n","# print(x_val.shape)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    MyDataset(torch.from_numpy(train_data.reshape(2700, 512, 1, 1)), torch.from_numpy(train_label)),\n","    batch_size=args['batch_size'], shuffle=True, **kwargs)\n","\n","# test_loader = torch.utils.data.DataLoader(\n","#     MyDataset(torch.from_numpy(x_val), torch.from_numpy(y_val)),\n","#     batch_size=args['test_batch_size'], shuffle=True, **kwargs)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CibRu6Zb7FE4","executionInfo":{"status":"ok","timestamp":1621836121104,"user_tz":-540,"elapsed":86493,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"40847085-334d-43c7-9cef-f56be6774896"},"source":["model = Net()\n","print(model)\n","#gpu있으면 gpu 사용\n","if args['cuda']:\n","    model.cuda()\n","\n","#SGD사용해 학습함\n","optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n","\n","for epoch in range(1, args['epochs'] + 1):\n","    train(epoch)\n","    #test()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2): Conv2d(512, 200, kernel_size=(1, 1), stride=(1, 1))\n","  (fc2): Linear(in_features=512, out_features=200, bias=True)\n",")\n","Train Epoch: 1 [0/2700 (0%)]\tLoss: 5.492162\n","Train Epoch: 1 [640/2700 (23%)]\tLoss: 5.476598\n","Train Epoch: 1 [1280/2700 (47%)]\tLoss: 5.313179\n","Train Epoch: 1 [1920/2700 (70%)]\tLoss: 5.375359\n","Train Epoch: 1 [2560/2700 (93%)]\tLoss: 5.343523\n","Train Epoch: 2 [0/2700 (0%)]\tLoss: 4.868478\n","Train Epoch: 2 [640/2700 (23%)]\tLoss: 4.699032\n","Train Epoch: 2 [1280/2700 (47%)]\tLoss: 4.614550\n","Train Epoch: 2 [1920/2700 (70%)]\tLoss: 4.429089\n","Train Epoch: 2 [2560/2700 (93%)]\tLoss: 4.463514\n","Train Epoch: 3 [0/2700 (0%)]\tLoss: 4.288553\n","Train Epoch: 3 [640/2700 (23%)]\tLoss: 3.941338\n","Train Epoch: 3 [1280/2700 (47%)]\tLoss: 4.026500\n","Train Epoch: 3 [1920/2700 (70%)]\tLoss: 4.002997\n","Train Epoch: 3 [2560/2700 (93%)]\tLoss: 3.954646\n","Train Epoch: 4 [0/2700 (0%)]\tLoss: 3.606663\n","Train Epoch: 4 [640/2700 (23%)]\tLoss: 3.668820\n","Train Epoch: 4 [1280/2700 (47%)]\tLoss: 3.566913\n","Train Epoch: 4 [1920/2700 (70%)]\tLoss: 3.572412\n","Train Epoch: 4 [2560/2700 (93%)]\tLoss: 3.616346\n","Train Epoch: 5 [0/2700 (0%)]\tLoss: 3.336120\n","Train Epoch: 5 [640/2700 (23%)]\tLoss: 3.191533\n","Train Epoch: 5 [1280/2700 (47%)]\tLoss: 2.715103\n","Train Epoch: 5 [1920/2700 (70%)]\tLoss: 3.029787\n","Train Epoch: 5 [2560/2700 (93%)]\tLoss: 3.141235\n","Train Epoch: 6 [0/2700 (0%)]\tLoss: 2.886014\n","Train Epoch: 6 [640/2700 (23%)]\tLoss: 2.610745\n","Train Epoch: 6 [1280/2700 (47%)]\tLoss: 2.665506\n","Train Epoch: 6 [1920/2700 (70%)]\tLoss: 2.540421\n","Train Epoch: 6 [2560/2700 (93%)]\tLoss: 2.503655\n","Train Epoch: 7 [0/2700 (0%)]\tLoss: 2.188183\n","Train Epoch: 7 [640/2700 (23%)]\tLoss: 2.240088\n","Train Epoch: 7 [1280/2700 (47%)]\tLoss: 2.082177\n","Train Epoch: 7 [1920/2700 (70%)]\tLoss: 2.285374\n","Train Epoch: 7 [2560/2700 (93%)]\tLoss: 2.165618\n","Train Epoch: 8 [0/2700 (0%)]\tLoss: 1.641712\n","Train Epoch: 8 [640/2700 (23%)]\tLoss: 1.715449\n","Train Epoch: 8 [1280/2700 (47%)]\tLoss: 1.733751\n","Train Epoch: 8 [1920/2700 (70%)]\tLoss: 1.664575\n","Train Epoch: 8 [2560/2700 (93%)]\tLoss: 1.847469\n","Train Epoch: 9 [0/2700 (0%)]\tLoss: 1.498475\n","Train Epoch: 9 [640/2700 (23%)]\tLoss: 1.397596\n","Train Epoch: 9 [1280/2700 (47%)]\tLoss: 1.341242\n","Train Epoch: 9 [1920/2700 (70%)]\tLoss: 1.444327\n","Train Epoch: 9 [2560/2700 (93%)]\tLoss: 1.406002\n","Train Epoch: 10 [0/2700 (0%)]\tLoss: 1.155127\n","Train Epoch: 10 [640/2700 (23%)]\tLoss: 1.151410\n","Train Epoch: 10 [1280/2700 (47%)]\tLoss: 0.988226\n","Train Epoch: 10 [1920/2700 (70%)]\tLoss: 1.103148\n","Train Epoch: 10 [2560/2700 (93%)]\tLoss: 1.082310\n","Train Epoch: 11 [0/2700 (0%)]\tLoss: 0.910835\n","Train Epoch: 11 [640/2700 (23%)]\tLoss: 0.752816\n","Train Epoch: 11 [1280/2700 (47%)]\tLoss: 0.813073\n","Train Epoch: 11 [1920/2700 (70%)]\tLoss: 0.952241\n","Train Epoch: 11 [2560/2700 (93%)]\tLoss: 0.870572\n","Train Epoch: 12 [0/2700 (0%)]\tLoss: 0.713828\n","Train Epoch: 12 [640/2700 (23%)]\tLoss: 0.703090\n","Train Epoch: 12 [1280/2700 (47%)]\tLoss: 0.654324\n","Train Epoch: 12 [1920/2700 (70%)]\tLoss: 0.726783\n","Train Epoch: 12 [2560/2700 (93%)]\tLoss: 0.718718\n","Train Epoch: 13 [0/2700 (0%)]\tLoss: 0.576992\n","Train Epoch: 13 [640/2700 (23%)]\tLoss: 0.486095\n","Train Epoch: 13 [1280/2700 (47%)]\tLoss: 0.543214\n","Train Epoch: 13 [1920/2700 (70%)]\tLoss: 0.592010\n","Train Epoch: 13 [2560/2700 (93%)]\tLoss: 0.467868\n","Train Epoch: 14 [0/2700 (0%)]\tLoss: 0.406778\n","Train Epoch: 14 [640/2700 (23%)]\tLoss: 0.407756\n","Train Epoch: 14 [1280/2700 (47%)]\tLoss: 0.378569\n","Train Epoch: 14 [1920/2700 (70%)]\tLoss: 0.408241\n","Train Epoch: 14 [2560/2700 (93%)]\tLoss: 0.525772\n","Train Epoch: 15 [0/2700 (0%)]\tLoss: 0.302658\n","Train Epoch: 15 [640/2700 (23%)]\tLoss: 0.376334\n","Train Epoch: 15 [1280/2700 (47%)]\tLoss: 0.385393\n","Train Epoch: 15 [1920/2700 (70%)]\tLoss: 0.329619\n","Train Epoch: 15 [2560/2700 (93%)]\tLoss: 0.415213\n","Train Epoch: 16 [0/2700 (0%)]\tLoss: 0.336048\n","Train Epoch: 16 [640/2700 (23%)]\tLoss: 0.347352\n","Train Epoch: 16 [1280/2700 (47%)]\tLoss: 0.291313\n","Train Epoch: 16 [1920/2700 (70%)]\tLoss: 0.320399\n","Train Epoch: 16 [2560/2700 (93%)]\tLoss: 0.303625\n","Train Epoch: 17 [0/2700 (0%)]\tLoss: 0.237681\n","Train Epoch: 17 [640/2700 (23%)]\tLoss: 0.262813\n","Train Epoch: 17 [1280/2700 (47%)]\tLoss: 0.291191\n","Train Epoch: 17 [1920/2700 (70%)]\tLoss: 0.244484\n","Train Epoch: 17 [2560/2700 (93%)]\tLoss: 0.274549\n","Train Epoch: 18 [0/2700 (0%)]\tLoss: 0.211124\n","Train Epoch: 18 [640/2700 (23%)]\tLoss: 0.208249\n","Train Epoch: 18 [1280/2700 (47%)]\tLoss: 0.187414\n","Train Epoch: 18 [1920/2700 (70%)]\tLoss: 0.257304\n","Train Epoch: 18 [2560/2700 (93%)]\tLoss: 0.204810\n","Train Epoch: 19 [0/2700 (0%)]\tLoss: 0.152650\n","Train Epoch: 19 [640/2700 (23%)]\tLoss: 0.204401\n","Train Epoch: 19 [1280/2700 (47%)]\tLoss: 0.213620\n","Train Epoch: 19 [1920/2700 (70%)]\tLoss: 0.247093\n","Train Epoch: 19 [2560/2700 (93%)]\tLoss: 0.184432\n","Train Epoch: 20 [0/2700 (0%)]\tLoss: 0.178802\n","Train Epoch: 20 [640/2700 (23%)]\tLoss: 0.152289\n","Train Epoch: 20 [1280/2700 (47%)]\tLoss: 0.153117\n","Train Epoch: 20 [1920/2700 (70%)]\tLoss: 0.161171\n","Train Epoch: 20 [2560/2700 (93%)]\tLoss: 0.131237\n","Train Epoch: 21 [0/2700 (0%)]\tLoss: 0.132063\n","Train Epoch: 21 [640/2700 (23%)]\tLoss: 0.190130\n","Train Epoch: 21 [1280/2700 (47%)]\tLoss: 0.125544\n","Train Epoch: 21 [1920/2700 (70%)]\tLoss: 0.141382\n","Train Epoch: 21 [2560/2700 (93%)]\tLoss: 0.155667\n","Train Epoch: 22 [0/2700 (0%)]\tLoss: 0.094580\n","Train Epoch: 22 [640/2700 (23%)]\tLoss: 0.104801\n","Train Epoch: 22 [1280/2700 (47%)]\tLoss: 0.123315\n","Train Epoch: 22 [1920/2700 (70%)]\tLoss: 0.176359\n","Train Epoch: 22 [2560/2700 (93%)]\tLoss: 0.134561\n","Train Epoch: 23 [0/2700 (0%)]\tLoss: 0.089079\n","Train Epoch: 23 [640/2700 (23%)]\tLoss: 0.104478\n","Train Epoch: 23 [1280/2700 (47%)]\tLoss: 0.123151\n","Train Epoch: 23 [1920/2700 (70%)]\tLoss: 0.100076\n","Train Epoch: 23 [2560/2700 (93%)]\tLoss: 0.107483\n","Train Epoch: 24 [0/2700 (0%)]\tLoss: 0.103230\n","Train Epoch: 24 [640/2700 (23%)]\tLoss: 0.088987\n","Train Epoch: 24 [1280/2700 (47%)]\tLoss: 0.104153\n","Train Epoch: 24 [1920/2700 (70%)]\tLoss: 0.131511\n","Train Epoch: 24 [2560/2700 (93%)]\tLoss: 0.118110\n","Train Epoch: 25 [0/2700 (0%)]\tLoss: 0.093355\n","Train Epoch: 25 [640/2700 (23%)]\tLoss: 0.070424\n","Train Epoch: 25 [1280/2700 (47%)]\tLoss: 0.069475\n","Train Epoch: 25 [1920/2700 (70%)]\tLoss: 0.078743\n","Train Epoch: 25 [2560/2700 (93%)]\tLoss: 0.110001\n","Train Epoch: 26 [0/2700 (0%)]\tLoss: 0.066224\n","Train Epoch: 26 [640/2700 (23%)]\tLoss: 0.058931\n","Train Epoch: 26 [1280/2700 (47%)]\tLoss: 0.084307\n","Train Epoch: 26 [1920/2700 (70%)]\tLoss: 0.082912\n","Train Epoch: 26 [2560/2700 (93%)]\tLoss: 0.066548\n","Train Epoch: 27 [0/2700 (0%)]\tLoss: 0.072742\n","Train Epoch: 27 [640/2700 (23%)]\tLoss: 0.086490\n","Train Epoch: 27 [1280/2700 (47%)]\tLoss: 0.091464\n","Train Epoch: 27 [1920/2700 (70%)]\tLoss: 0.084997\n","Train Epoch: 27 [2560/2700 (93%)]\tLoss: 0.087968\n","Train Epoch: 28 [0/2700 (0%)]\tLoss: 0.071720\n","Train Epoch: 28 [640/2700 (23%)]\tLoss: 0.067016\n","Train Epoch: 28 [1280/2700 (47%)]\tLoss: 0.062275\n","Train Epoch: 28 [1920/2700 (70%)]\tLoss: 0.084116\n","Train Epoch: 28 [2560/2700 (93%)]\tLoss: 0.092760\n","Train Epoch: 29 [0/2700 (0%)]\tLoss: 0.074719\n","Train Epoch: 29 [640/2700 (23%)]\tLoss: 0.081815\n","Train Epoch: 29 [1280/2700 (47%)]\tLoss: 0.055834\n","Train Epoch: 29 [1920/2700 (70%)]\tLoss: 0.074691\n","Train Epoch: 29 [2560/2700 (93%)]\tLoss: 0.082428\n","Train Epoch: 30 [0/2700 (0%)]\tLoss: 0.070836\n","Train Epoch: 30 [640/2700 (23%)]\tLoss: 0.080990\n","Train Epoch: 30 [1280/2700 (47%)]\tLoss: 0.075015\n","Train Epoch: 30 [1920/2700 (70%)]\tLoss: 0.089937\n","Train Epoch: 30 [2560/2700 (93%)]\tLoss: 0.045464\n","Train Epoch: 31 [0/2700 (0%)]\tLoss: 0.058044\n","Train Epoch: 31 [640/2700 (23%)]\tLoss: 0.065853\n","Train Epoch: 31 [1280/2700 (47%)]\tLoss: 0.069398\n","Train Epoch: 31 [1920/2700 (70%)]\tLoss: 0.053902\n","Train Epoch: 31 [2560/2700 (93%)]\tLoss: 0.044624\n","Train Epoch: 32 [0/2700 (0%)]\tLoss: 0.054856\n","Train Epoch: 32 [640/2700 (23%)]\tLoss: 0.045729\n","Train Epoch: 32 [1280/2700 (47%)]\tLoss: 0.050500\n","Train Epoch: 32 [1920/2700 (70%)]\tLoss: 0.053416\n","Train Epoch: 32 [2560/2700 (93%)]\tLoss: 0.074617\n","Train Epoch: 33 [0/2700 (0%)]\tLoss: 0.046531\n","Train Epoch: 33 [640/2700 (23%)]\tLoss: 0.046584\n","Train Epoch: 33 [1280/2700 (47%)]\tLoss: 0.037461\n","Train Epoch: 33 [1920/2700 (70%)]\tLoss: 0.034627\n","Train Epoch: 33 [2560/2700 (93%)]\tLoss: 0.046966\n","Train Epoch: 34 [0/2700 (0%)]\tLoss: 0.032944\n","Train Epoch: 34 [640/2700 (23%)]\tLoss: 0.053494\n","Train Epoch: 34 [1280/2700 (47%)]\tLoss: 0.057499\n","Train Epoch: 34 [1920/2700 (70%)]\tLoss: 0.063965\n","Train Epoch: 34 [2560/2700 (93%)]\tLoss: 0.042968\n","Train Epoch: 35 [0/2700 (0%)]\tLoss: 0.040293\n","Train Epoch: 35 [640/2700 (23%)]\tLoss: 0.049048\n","Train Epoch: 35 [1280/2700 (47%)]\tLoss: 0.053241\n","Train Epoch: 35 [1920/2700 (70%)]\tLoss: 0.065245\n","Train Epoch: 35 [2560/2700 (93%)]\tLoss: 0.042242\n","Train Epoch: 36 [0/2700 (0%)]\tLoss: 0.037633\n","Train Epoch: 36 [640/2700 (23%)]\tLoss: 0.047901\n","Train Epoch: 36 [1280/2700 (47%)]\tLoss: 0.060586\n","Train Epoch: 36 [1920/2700 (70%)]\tLoss: 0.044714\n","Train Epoch: 36 [2560/2700 (93%)]\tLoss: 0.040879\n","Train Epoch: 37 [0/2700 (0%)]\tLoss: 0.044403\n","Train Epoch: 37 [640/2700 (23%)]\tLoss: 0.035580\n","Train Epoch: 37 [1280/2700 (47%)]\tLoss: 0.039956\n","Train Epoch: 37 [1920/2700 (70%)]\tLoss: 0.036290\n","Train Epoch: 37 [2560/2700 (93%)]\tLoss: 0.042842\n","Train Epoch: 38 [0/2700 (0%)]\tLoss: 0.029330\n","Train Epoch: 38 [640/2700 (23%)]\tLoss: 0.041471\n","Train Epoch: 38 [1280/2700 (47%)]\tLoss: 0.063505\n","Train Epoch: 38 [1920/2700 (70%)]\tLoss: 0.050115\n","Train Epoch: 38 [2560/2700 (93%)]\tLoss: 0.053416\n","Train Epoch: 39 [0/2700 (0%)]\tLoss: 0.041420\n","Train Epoch: 39 [640/2700 (23%)]\tLoss: 0.018684\n","Train Epoch: 39 [1280/2700 (47%)]\tLoss: 0.044307\n","Train Epoch: 39 [1920/2700 (70%)]\tLoss: 0.039555\n","Train Epoch: 39 [2560/2700 (93%)]\tLoss: 0.058321\n","Train Epoch: 40 [0/2700 (0%)]\tLoss: 0.045094\n","Train Epoch: 40 [640/2700 (23%)]\tLoss: 0.035427\n","Train Epoch: 40 [1280/2700 (47%)]\tLoss: 0.040230\n","Train Epoch: 40 [1920/2700 (70%)]\tLoss: 0.045371\n","Train Epoch: 40 [2560/2700 (93%)]\tLoss: 0.031524\n","Train Epoch: 41 [0/2700 (0%)]\tLoss: 0.035054\n","Train Epoch: 41 [640/2700 (23%)]\tLoss: 0.042170\n","Train Epoch: 41 [1280/2700 (47%)]\tLoss: 0.026517\n","Train Epoch: 41 [1920/2700 (70%)]\tLoss: 0.027596\n","Train Epoch: 41 [2560/2700 (93%)]\tLoss: 0.022336\n","Train Epoch: 42 [0/2700 (0%)]\tLoss: 0.034792\n","Train Epoch: 42 [640/2700 (23%)]\tLoss: 0.025955\n","Train Epoch: 42 [1280/2700 (47%)]\tLoss: 0.034465\n","Train Epoch: 42 [1920/2700 (70%)]\tLoss: 0.027925\n","Train Epoch: 42 [2560/2700 (93%)]\tLoss: 0.035488\n","Train Epoch: 43 [0/2700 (0%)]\tLoss: 0.041074\n","Train Epoch: 43 [640/2700 (23%)]\tLoss: 0.019900\n","Train Epoch: 43 [1280/2700 (47%)]\tLoss: 0.037529\n","Train Epoch: 43 [1920/2700 (70%)]\tLoss: 0.019170\n","Train Epoch: 43 [2560/2700 (93%)]\tLoss: 0.032242\n","Train Epoch: 44 [0/2700 (0%)]\tLoss: 0.031529\n","Train Epoch: 44 [640/2700 (23%)]\tLoss: 0.026412\n","Train Epoch: 44 [1280/2700 (47%)]\tLoss: 0.034301\n","Train Epoch: 44 [1920/2700 (70%)]\tLoss: 0.017286\n","Train Epoch: 44 [2560/2700 (93%)]\tLoss: 0.030955\n","Train Epoch: 45 [0/2700 (0%)]\tLoss: 0.033326\n","Train Epoch: 45 [640/2700 (23%)]\tLoss: 0.021444\n","Train Epoch: 45 [1280/2700 (47%)]\tLoss: 0.035241\n","Train Epoch: 45 [1920/2700 (70%)]\tLoss: 0.014741\n","Train Epoch: 45 [2560/2700 (93%)]\tLoss: 0.026071\n","Train Epoch: 46 [0/2700 (0%)]\tLoss: 0.030082\n","Train Epoch: 46 [640/2700 (23%)]\tLoss: 0.016786\n","Train Epoch: 46 [1280/2700 (47%)]\tLoss: 0.025845\n","Train Epoch: 46 [1920/2700 (70%)]\tLoss: 0.022958\n","Train Epoch: 46 [2560/2700 (93%)]\tLoss: 0.027564\n","Train Epoch: 47 [0/2700 (0%)]\tLoss: 0.023714\n","Train Epoch: 47 [640/2700 (23%)]\tLoss: 0.017908\n","Train Epoch: 47 [1280/2700 (47%)]\tLoss: 0.025005\n","Train Epoch: 47 [1920/2700 (70%)]\tLoss: 0.016447\n","Train Epoch: 47 [2560/2700 (93%)]\tLoss: 0.011602\n","Train Epoch: 48 [0/2700 (0%)]\tLoss: 0.032473\n","Train Epoch: 48 [640/2700 (23%)]\tLoss: 0.024632\n","Train Epoch: 48 [1280/2700 (47%)]\tLoss: 0.021899\n","Train Epoch: 48 [1920/2700 (70%)]\tLoss: 0.017013\n","Train Epoch: 48 [2560/2700 (93%)]\tLoss: 0.014912\n","Train Epoch: 49 [0/2700 (0%)]\tLoss: 0.023018\n","Train Epoch: 49 [640/2700 (23%)]\tLoss: 0.016073\n","Train Epoch: 49 [1280/2700 (47%)]\tLoss: 0.013861\n","Train Epoch: 49 [1920/2700 (70%)]\tLoss: 0.026882\n","Train Epoch: 49 [2560/2700 (93%)]\tLoss: 0.029505\n","Train Epoch: 50 [0/2700 (0%)]\tLoss: 0.028636\n","Train Epoch: 50 [640/2700 (23%)]\tLoss: 0.012525\n","Train Epoch: 50 [1280/2700 (47%)]\tLoss: 0.020721\n","Train Epoch: 50 [1920/2700 (70%)]\tLoss: 0.014943\n","Train Epoch: 50 [2560/2700 (93%)]\tLoss: 0.021752\n","Train Epoch: 51 [0/2700 (0%)]\tLoss: 0.014659\n","Train Epoch: 51 [640/2700 (23%)]\tLoss: 0.014301\n","Train Epoch: 51 [1280/2700 (47%)]\tLoss: 0.019026\n","Train Epoch: 51 [1920/2700 (70%)]\tLoss: 0.026344\n","Train Epoch: 51 [2560/2700 (93%)]\tLoss: 0.017831\n","Train Epoch: 52 [0/2700 (0%)]\tLoss: 0.024832\n","Train Epoch: 52 [640/2700 (23%)]\tLoss: 0.014658\n","Train Epoch: 52 [1280/2700 (47%)]\tLoss: 0.017203\n","Train Epoch: 52 [1920/2700 (70%)]\tLoss: 0.012613\n","Train Epoch: 52 [2560/2700 (93%)]\tLoss: 0.029915\n","Train Epoch: 53 [0/2700 (0%)]\tLoss: 0.021385\n","Train Epoch: 53 [640/2700 (23%)]\tLoss: 0.017193\n","Train Epoch: 53 [1280/2700 (47%)]\tLoss: 0.015476\n","Train Epoch: 53 [1920/2700 (70%)]\tLoss: 0.018565\n","Train Epoch: 53 [2560/2700 (93%)]\tLoss: 0.025267\n","Train Epoch: 54 [0/2700 (0%)]\tLoss: 0.026721\n","Train Epoch: 54 [640/2700 (23%)]\tLoss: 0.029211\n","Train Epoch: 54 [1280/2700 (47%)]\tLoss: 0.024885\n","Train Epoch: 54 [1920/2700 (70%)]\tLoss: 0.019163\n","Train Epoch: 54 [2560/2700 (93%)]\tLoss: 0.014262\n","Train Epoch: 55 [0/2700 (0%)]\tLoss: 0.025836\n","Train Epoch: 55 [640/2700 (23%)]\tLoss: 0.029171\n","Train Epoch: 55 [1280/2700 (47%)]\tLoss: 0.021434\n","Train Epoch: 55 [1920/2700 (70%)]\tLoss: 0.017155\n","Train Epoch: 55 [2560/2700 (93%)]\tLoss: 0.019159\n","Train Epoch: 56 [0/2700 (0%)]\tLoss: 0.010391\n","Train Epoch: 56 [640/2700 (23%)]\tLoss: 0.015922\n","Train Epoch: 56 [1280/2700 (47%)]\tLoss: 0.009180\n","Train Epoch: 56 [1920/2700 (70%)]\tLoss: 0.017143\n","Train Epoch: 56 [2560/2700 (93%)]\tLoss: 0.017354\n","Train Epoch: 57 [0/2700 (0%)]\tLoss: 0.020316\n","Train Epoch: 57 [640/2700 (23%)]\tLoss: 0.023175\n","Train Epoch: 57 [1280/2700 (47%)]\tLoss: 0.012088\n","Train Epoch: 57 [1920/2700 (70%)]\tLoss: 0.013381\n","Train Epoch: 57 [2560/2700 (93%)]\tLoss: 0.034359\n","Train Epoch: 58 [0/2700 (0%)]\tLoss: 0.015170\n","Train Epoch: 58 [640/2700 (23%)]\tLoss: 0.015992\n","Train Epoch: 58 [1280/2700 (47%)]\tLoss: 0.016566\n","Train Epoch: 58 [1920/2700 (70%)]\tLoss: 0.010521\n","Train Epoch: 58 [2560/2700 (93%)]\tLoss: 0.028452\n","Train Epoch: 59 [0/2700 (0%)]\tLoss: 0.012648\n","Train Epoch: 59 [640/2700 (23%)]\tLoss: 0.016764\n","Train Epoch: 59 [1280/2700 (47%)]\tLoss: 0.010369\n","Train Epoch: 59 [1920/2700 (70%)]\tLoss: 0.010199\n","Train Epoch: 59 [2560/2700 (93%)]\tLoss: 0.021381\n","Train Epoch: 60 [0/2700 (0%)]\tLoss: 0.013108\n","Train Epoch: 60 [640/2700 (23%)]\tLoss: 0.011093\n","Train Epoch: 60 [1280/2700 (47%)]\tLoss: 0.010129\n","Train Epoch: 60 [1920/2700 (70%)]\tLoss: 0.020140\n","Train Epoch: 60 [2560/2700 (93%)]\tLoss: 0.011672\n","Train Epoch: 61 [0/2700 (0%)]\tLoss: 0.014918\n","Train Epoch: 61 [640/2700 (23%)]\tLoss: 0.012394\n","Train Epoch: 61 [1280/2700 (47%)]\tLoss: 0.016405\n","Train Epoch: 61 [1920/2700 (70%)]\tLoss: 0.022419\n","Train Epoch: 61 [2560/2700 (93%)]\tLoss: 0.024247\n","Train Epoch: 62 [0/2700 (0%)]\tLoss: 0.013833\n","Train Epoch: 62 [640/2700 (23%)]\tLoss: 0.012453\n","Train Epoch: 62 [1280/2700 (47%)]\tLoss: 0.013098\n","Train Epoch: 62 [1920/2700 (70%)]\tLoss: 0.011951\n","Train Epoch: 62 [2560/2700 (93%)]\tLoss: 0.011651\n","Train Epoch: 63 [0/2700 (0%)]\tLoss: 0.019739\n","Train Epoch: 63 [640/2700 (23%)]\tLoss: 0.013740\n","Train Epoch: 63 [1280/2700 (47%)]\tLoss: 0.014386\n","Train Epoch: 63 [1920/2700 (70%)]\tLoss: 0.014891\n","Train Epoch: 63 [2560/2700 (93%)]\tLoss: 0.009799\n","Train Epoch: 64 [0/2700 (0%)]\tLoss: 0.010886\n","Train Epoch: 64 [640/2700 (23%)]\tLoss: 0.010900\n","Train Epoch: 64 [1280/2700 (47%)]\tLoss: 0.016859\n","Train Epoch: 64 [1920/2700 (70%)]\tLoss: 0.016239\n","Train Epoch: 64 [2560/2700 (93%)]\tLoss: 0.016315\n","Train Epoch: 65 [0/2700 (0%)]\tLoss: 0.015200\n","Train Epoch: 65 [640/2700 (23%)]\tLoss: 0.008894\n","Train Epoch: 65 [1280/2700 (47%)]\tLoss: 0.009368\n","Train Epoch: 65 [1920/2700 (70%)]\tLoss: 0.010826\n","Train Epoch: 65 [2560/2700 (93%)]\tLoss: 0.012103\n","Train Epoch: 66 [0/2700 (0%)]\tLoss: 0.011850\n","Train Epoch: 66 [640/2700 (23%)]\tLoss: 0.010155\n","Train Epoch: 66 [1280/2700 (47%)]\tLoss: 0.011598\n","Train Epoch: 66 [1920/2700 (70%)]\tLoss: 0.029242\n","Train Epoch: 66 [2560/2700 (93%)]\tLoss: 0.010464\n","Train Epoch: 67 [0/2700 (0%)]\tLoss: 0.008447\n","Train Epoch: 67 [640/2700 (23%)]\tLoss: 0.006387\n","Train Epoch: 67 [1280/2700 (47%)]\tLoss: 0.006800\n","Train Epoch: 67 [1920/2700 (70%)]\tLoss: 0.007064\n","Train Epoch: 67 [2560/2700 (93%)]\tLoss: 0.007654\n","Train Epoch: 68 [0/2700 (0%)]\tLoss: 0.007806\n","Train Epoch: 68 [640/2700 (23%)]\tLoss: 0.013419\n","Train Epoch: 68 [1280/2700 (47%)]\tLoss: 0.013011\n","Train Epoch: 68 [1920/2700 (70%)]\tLoss: 0.008983\n","Train Epoch: 68 [2560/2700 (93%)]\tLoss: 0.012844\n","Train Epoch: 69 [0/2700 (0%)]\tLoss: 0.007757\n","Train Epoch: 69 [640/2700 (23%)]\tLoss: 0.012242\n","Train Epoch: 69 [1280/2700 (47%)]\tLoss: 0.017902\n","Train Epoch: 69 [1920/2700 (70%)]\tLoss: 0.006828\n","Train Epoch: 69 [2560/2700 (93%)]\tLoss: 0.011140\n","Train Epoch: 70 [0/2700 (0%)]\tLoss: 0.008223\n","Train Epoch: 70 [640/2700 (23%)]\tLoss: 0.012354\n","Train Epoch: 70 [1280/2700 (47%)]\tLoss: 0.009286\n","Train Epoch: 70 [1920/2700 (70%)]\tLoss: 0.012295\n","Train Epoch: 70 [2560/2700 (93%)]\tLoss: 0.020217\n","Train Epoch: 71 [0/2700 (0%)]\tLoss: 0.009817\n","Train Epoch: 71 [640/2700 (23%)]\tLoss: 0.011614\n","Train Epoch: 71 [1280/2700 (47%)]\tLoss: 0.015604\n","Train Epoch: 71 [1920/2700 (70%)]\tLoss: 0.009204\n","Train Epoch: 71 [2560/2700 (93%)]\tLoss: 0.007472\n","Train Epoch: 72 [0/2700 (0%)]\tLoss: 0.009814\n","Train Epoch: 72 [640/2700 (23%)]\tLoss: 0.009466\n","Train Epoch: 72 [1280/2700 (47%)]\tLoss: 0.008875\n","Train Epoch: 72 [1920/2700 (70%)]\tLoss: 0.006242\n","Train Epoch: 72 [2560/2700 (93%)]\tLoss: 0.014118\n","Train Epoch: 73 [0/2700 (0%)]\tLoss: 0.007012\n","Train Epoch: 73 [640/2700 (23%)]\tLoss: 0.011649\n","Train Epoch: 73 [1280/2700 (47%)]\tLoss: 0.006579\n","Train Epoch: 73 [1920/2700 (70%)]\tLoss: 0.009461\n","Train Epoch: 73 [2560/2700 (93%)]\tLoss: 0.008996\n","Train Epoch: 74 [0/2700 (0%)]\tLoss: 0.004716\n","Train Epoch: 74 [640/2700 (23%)]\tLoss: 0.006723\n","Train Epoch: 74 [1280/2700 (47%)]\tLoss: 0.019690\n","Train Epoch: 74 [1920/2700 (70%)]\tLoss: 0.011141\n","Train Epoch: 74 [2560/2700 (93%)]\tLoss: 0.007779\n","Train Epoch: 75 [0/2700 (0%)]\tLoss: 0.007690\n","Train Epoch: 75 [640/2700 (23%)]\tLoss: 0.005948\n","Train Epoch: 75 [1280/2700 (47%)]\tLoss: 0.008032\n","Train Epoch: 75 [1920/2700 (70%)]\tLoss: 0.017906\n","Train Epoch: 75 [2560/2700 (93%)]\tLoss: 0.013247\n","Train Epoch: 76 [0/2700 (0%)]\tLoss: 0.009618\n","Train Epoch: 76 [640/2700 (23%)]\tLoss: 0.018317\n","Train Epoch: 76 [1280/2700 (47%)]\tLoss: 0.010376\n","Train Epoch: 76 [1920/2700 (70%)]\tLoss: 0.013036\n","Train Epoch: 76 [2560/2700 (93%)]\tLoss: 0.007591\n","Train Epoch: 77 [0/2700 (0%)]\tLoss: 0.006173\n","Train Epoch: 77 [640/2700 (23%)]\tLoss: 0.007344\n","Train Epoch: 77 [1280/2700 (47%)]\tLoss: 0.007195\n","Train Epoch: 77 [1920/2700 (70%)]\tLoss: 0.010436\n","Train Epoch: 77 [2560/2700 (93%)]\tLoss: 0.010696\n","Train Epoch: 78 [0/2700 (0%)]\tLoss: 0.012945\n","Train Epoch: 78 [640/2700 (23%)]\tLoss: 0.011722\n","Train Epoch: 78 [1280/2700 (47%)]\tLoss: 0.007793\n","Train Epoch: 78 [1920/2700 (70%)]\tLoss: 0.007050\n","Train Epoch: 78 [2560/2700 (93%)]\tLoss: 0.005452\n","Train Epoch: 79 [0/2700 (0%)]\tLoss: 0.005748\n","Train Epoch: 79 [640/2700 (23%)]\tLoss: 0.006434\n","Train Epoch: 79 [1280/2700 (47%)]\tLoss: 0.005049\n","Train Epoch: 79 [1920/2700 (70%)]\tLoss: 0.008822\n","Train Epoch: 79 [2560/2700 (93%)]\tLoss: 0.009238\n","Train Epoch: 80 [0/2700 (0%)]\tLoss: 0.006804\n","Train Epoch: 80 [640/2700 (23%)]\tLoss: 0.007869\n","Train Epoch: 80 [1280/2700 (47%)]\tLoss: 0.017653\n","Train Epoch: 80 [1920/2700 (70%)]\tLoss: 0.008637\n","Train Epoch: 80 [2560/2700 (93%)]\tLoss: 0.007676\n","Train Epoch: 81 [0/2700 (0%)]\tLoss: 0.006471\n","Train Epoch: 81 [640/2700 (23%)]\tLoss: 0.007625\n","Train Epoch: 81 [1280/2700 (47%)]\tLoss: 0.009489\n","Train Epoch: 81 [1920/2700 (70%)]\tLoss: 0.007389\n","Train Epoch: 81 [2560/2700 (93%)]\tLoss: 0.007465\n","Train Epoch: 82 [0/2700 (0%)]\tLoss: 0.007010\n","Train Epoch: 82 [640/2700 (23%)]\tLoss: 0.011625\n","Train Epoch: 82 [1280/2700 (47%)]\tLoss: 0.007578\n","Train Epoch: 82 [1920/2700 (70%)]\tLoss: 0.005255\n","Train Epoch: 82 [2560/2700 (93%)]\tLoss: 0.005012\n","Train Epoch: 83 [0/2700 (0%)]\tLoss: 0.005853\n","Train Epoch: 83 [640/2700 (23%)]\tLoss: 0.008260\n","Train Epoch: 83 [1280/2700 (47%)]\tLoss: 0.009143\n","Train Epoch: 83 [1920/2700 (70%)]\tLoss: 0.008236\n","Train Epoch: 83 [2560/2700 (93%)]\tLoss: 0.006496\n","Train Epoch: 84 [0/2700 (0%)]\tLoss: 0.006572\n","Train Epoch: 84 [640/2700 (23%)]\tLoss: 0.007038\n","Train Epoch: 84 [1280/2700 (47%)]\tLoss: 0.009112\n","Train Epoch: 84 [1920/2700 (70%)]\tLoss: 0.010836\n","Train Epoch: 84 [2560/2700 (93%)]\tLoss: 0.004354\n","Train Epoch: 85 [0/2700 (0%)]\tLoss: 0.013848\n","Train Epoch: 85 [640/2700 (23%)]\tLoss: 0.015068\n","Train Epoch: 85 [1280/2700 (47%)]\tLoss: 0.004808\n","Train Epoch: 85 [1920/2700 (70%)]\tLoss: 0.006608\n","Train Epoch: 85 [2560/2700 (93%)]\tLoss: 0.002843\n","Train Epoch: 86 [0/2700 (0%)]\tLoss: 0.003479\n","Train Epoch: 86 [640/2700 (23%)]\tLoss: 0.005682\n","Train Epoch: 86 [1280/2700 (47%)]\tLoss: 0.005764\n","Train Epoch: 86 [1920/2700 (70%)]\tLoss: 0.005276\n","Train Epoch: 86 [2560/2700 (93%)]\tLoss: 0.006548\n","Train Epoch: 87 [0/2700 (0%)]\tLoss: 0.005589\n","Train Epoch: 87 [640/2700 (23%)]\tLoss: 0.008375\n","Train Epoch: 87 [1280/2700 (47%)]\tLoss: 0.006314\n","Train Epoch: 87 [1920/2700 (70%)]\tLoss: 0.005378\n","Train Epoch: 87 [2560/2700 (93%)]\tLoss: 0.005784\n","Train Epoch: 88 [0/2700 (0%)]\tLoss: 0.004428\n","Train Epoch: 88 [640/2700 (23%)]\tLoss: 0.006686\n","Train Epoch: 88 [1280/2700 (47%)]\tLoss: 0.005779\n","Train Epoch: 88 [1920/2700 (70%)]\tLoss: 0.007226\n","Train Epoch: 88 [2560/2700 (93%)]\tLoss: 0.006326\n","Train Epoch: 89 [0/2700 (0%)]\tLoss: 0.003752\n","Train Epoch: 89 [640/2700 (23%)]\tLoss: 0.006120\n","Train Epoch: 89 [1280/2700 (47%)]\tLoss: 0.004831\n","Train Epoch: 89 [1920/2700 (70%)]\tLoss: 0.006774\n","Train Epoch: 89 [2560/2700 (93%)]\tLoss: 0.004301\n","Train Epoch: 90 [0/2700 (0%)]\tLoss: 0.005935\n","Train Epoch: 90 [640/2700 (23%)]\tLoss: 0.005154\n","Train Epoch: 90 [1280/2700 (47%)]\tLoss: 0.007404\n","Train Epoch: 90 [1920/2700 (70%)]\tLoss: 0.008673\n","Train Epoch: 90 [2560/2700 (93%)]\tLoss: 0.004019\n","Train Epoch: 91 [0/2700 (0%)]\tLoss: 0.004365\n","Train Epoch: 91 [640/2700 (23%)]\tLoss: 0.003879\n","Train Epoch: 91 [1280/2700 (47%)]\tLoss: 0.003890\n","Train Epoch: 91 [1920/2700 (70%)]\tLoss: 0.010375\n","Train Epoch: 91 [2560/2700 (93%)]\tLoss: 0.005164\n","Train Epoch: 92 [0/2700 (0%)]\tLoss: 0.006857\n","Train Epoch: 92 [640/2700 (23%)]\tLoss: 0.004689\n","Train Epoch: 92 [1280/2700 (47%)]\tLoss: 0.010265\n","Train Epoch: 92 [1920/2700 (70%)]\tLoss: 0.026312\n","Train Epoch: 92 [2560/2700 (93%)]\tLoss: 0.003966\n","Train Epoch: 93 [0/2700 (0%)]\tLoss: 0.010071\n","Train Epoch: 93 [640/2700 (23%)]\tLoss: 0.007580\n","Train Epoch: 93 [1280/2700 (47%)]\tLoss: 0.005868\n","Train Epoch: 93 [1920/2700 (70%)]\tLoss: 0.006339\n","Train Epoch: 93 [2560/2700 (93%)]\tLoss: 0.004935\n","Train Epoch: 94 [0/2700 (0%)]\tLoss: 0.005123\n","Train Epoch: 94 [640/2700 (23%)]\tLoss: 0.003835\n","Train Epoch: 94 [1280/2700 (47%)]\tLoss: 0.003291\n","Train Epoch: 94 [1920/2700 (70%)]\tLoss: 0.006532\n","Train Epoch: 94 [2560/2700 (93%)]\tLoss: 0.004195\n","Train Epoch: 95 [0/2700 (0%)]\tLoss: 0.007042\n","Train Epoch: 95 [640/2700 (23%)]\tLoss: 0.005328\n","Train Epoch: 95 [1280/2700 (47%)]\tLoss: 0.004122\n","Train Epoch: 95 [1920/2700 (70%)]\tLoss: 0.002510\n","Train Epoch: 95 [2560/2700 (93%)]\tLoss: 0.009428\n","Train Epoch: 96 [0/2700 (0%)]\tLoss: 0.005080\n","Train Epoch: 96 [640/2700 (23%)]\tLoss: 0.009836\n","Train Epoch: 96 [1280/2700 (47%)]\tLoss: 0.004898\n","Train Epoch: 96 [1920/2700 (70%)]\tLoss: 0.004244\n","Train Epoch: 96 [2560/2700 (93%)]\tLoss: 0.004427\n","Train Epoch: 97 [0/2700 (0%)]\tLoss: 0.004622\n","Train Epoch: 97 [640/2700 (23%)]\tLoss: 0.003749\n","Train Epoch: 97 [1280/2700 (47%)]\tLoss: 0.005145\n","Train Epoch: 97 [1920/2700 (70%)]\tLoss: 0.003997\n","Train Epoch: 97 [2560/2700 (93%)]\tLoss: 0.005322\n","Train Epoch: 98 [0/2700 (0%)]\tLoss: 0.005202\n","Train Epoch: 98 [640/2700 (23%)]\tLoss: 0.003905\n","Train Epoch: 98 [1280/2700 (47%)]\tLoss: 0.006420\n","Train Epoch: 98 [1920/2700 (70%)]\tLoss: 0.004147\n","Train Epoch: 98 [2560/2700 (93%)]\tLoss: 0.008549\n","Train Epoch: 99 [0/2700 (0%)]\tLoss: 0.004321\n","Train Epoch: 99 [640/2700 (23%)]\tLoss: 0.008069\n","Train Epoch: 99 [1280/2700 (47%)]\tLoss: 0.003821\n","Train Epoch: 99 [1920/2700 (70%)]\tLoss: 0.003939\n","Train Epoch: 99 [2560/2700 (93%)]\tLoss: 0.007186\n","Train Epoch: 100 [0/2700 (0%)]\tLoss: 0.005263\n","Train Epoch: 100 [640/2700 (23%)]\tLoss: 0.012131\n","Train Epoch: 100 [1280/2700 (47%)]\tLoss: 0.002531\n","Train Epoch: 100 [1920/2700 (70%)]\tLoss: 0.003682\n","Train Epoch: 100 [2560/2700 (93%)]\tLoss: 0.003368\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l8VWFKKc3V-N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621836121436,"user_tz":-540,"elapsed":347,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"6f8363ae-0893-4a75-cd4f-e7f8088df518"},"source":["v1_test_query_data_reshaped = np.reshape(v1_test_query_data,(v1_test_query_data.shape[0], v1_test_query_data.shape[2], v1_test_query_data.shape[3], v1_test_query_data.shape[4]))\n","# test_query_data_reshaped = np.reshape(test_query_data,(test_query_data.shape[0], test_query_data.shape[2], test_query_data.shape[3], test_query_data.shape[4]))\n","test_gallery_data_reshaped = np.reshape(test_gallery_data,(test_gallery_data.shape[0], test_gallery_data.shape[2], test_gallery_data.shape[3], test_gallery_data.shape[4]))\n","\n","# test_query_data_reshaped = test_query_data.reshape(500, 512, 1, 1)\n","# test_gallery_data_reshaped = test_gallery_data.reshape(100, 512, 1, 1)\n","\n","print(v1_test_query_data_reshaped.shape)\n","print(test_gallery_data_reshaped.shape)\n","print(v1_test_query_data.shape)\n","print(test_gallery_data.shape)\n","\n","v1_test_query_loader = torch.utils.data.DataLoader(\n","    MyDataset(torch.from_numpy(v1_test_query_data_reshaped)),\n","    batch_size=args['test_batch_size'], shuffle=False, **kwargs)\n","\n","# test_query_loader = torch.utils.data.DataLoader(\n","#     MyDataset(torch.from_numpy(test_query_data_reshaped)),\n","#     batch_size=args['test_batch_size'], shuffle=False, **kwargs)\n","\n","test_gallery_loader = torch.utils.data.DataLoader(\n","    MyDataset(torch.from_numpy(test_gallery_data_reshaped)),\n","    batch_size=args['test_batch_size'], shuffle=False, **kwargs)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["(1000, 512, 1, 1)\n","(100, 512, 1, 1)\n","(1000, 1, 512, 1, 1)\n","(100, 1, 512, 1, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aq225LaF6gue","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621836121437,"user_tz":-540,"elapsed":10,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"b258f7c7-deb9-4c65-d8b3-485196deead6"},"source":["import math\n","#테스트 갤러리랑 테스트 쿼리를 모델에 집어넣어서 feature를 뽑아낸 후, 테스트 갤러리랑 테스트 쿼리의 거리를 계산해 봄\n","#fc2 레이어를 무시할거기 떄문에 []:-2]로 함\n","feature_extractor = torch.nn.Sequential(*list(model.children())[:-2])\n","print(feature_extractor)\n","print(*list(model.children()))\n","print(*list(model.children())[:-2])\n","\n","#이걸 딥러닝에 넣음\n","#512차원을 200차원으로 바꾸는 거였는데, 그렇게 안하고 conv1만 처리함\n","# test_query_feature = np.zeros((test_query_data.shape[0], 512))\n","test_gallery_feature = np.zeros((test_gallery_data.shape[0], 512))\n","v1_test_query_feature = np.zeros((v1_test_query_data.shape[0], 512))\n","\n","\n","\n","# model train data <-> train label\n","# model <- query data = query label\n","# model <- gallery data = gallery label\n","\n","cursor = 0\n","for data in v1_test_query_loader:\n","  v1_test_query_feature[cursor:min(cursor+args['test_batch_size'], v1_test_query_data.shape[0]) ,:] = feature_extractor(data).detach().numpy().squeeze()\n","  cursor+=args['test_batch_size']\n","\n","# cursor = 0\n","# for data in test_query_loader:\n","#   test_query_feature[cursor:min(cursor+args['test_batch_size'], test_query_data.shape[0]) ,:] = feature_extractor(data).detach().numpy().squeeze()\n","#   cursor+=args['test_batch_size']\n","\n","cursor = 0\n","for data in test_gallery_loader:\n","  test_gallery_feature[cursor:min(cursor+args['test_batch_size'], test_gallery_data.shape[0]) ,:] = feature_extractor(data).detach().numpy().squeeze()\n","  cursor+=args['test_batch_size']\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Sequential(\n","  (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",")\n","Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1)) BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) Conv2d(512, 200, kernel_size=(1, 1), stride=(1, 1)) Linear(in_features=512, out_features=200, bias=True)\n","Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1)) BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"berTHxwAr-Kc","executionInfo":{"status":"ok","timestamp":1621836121438,"user_tz":-540,"elapsed":6,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["def normalized(a, axis=-1, order=2):\n","  l2=np.atleast_1d(np.linalg.norm(a,order,axis))\n","  l2[l2==0]=1\n","  return a/np.expand_dims(l2,axis)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"0uYWFrU19G6K","executionInfo":{"status":"ok","timestamp":1621836121438,"user_tz":-540,"elapsed":5,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["def getNearestNeibor(query, gallery, to=1):\n","  num_query = query.shape[0]\n","  num_gallery = gallery.shape[0]\n","  nn_idx = np.zeros(num_query)\n","  print(type(to))\n","  nn_idx2 = np.zeros((num_query, to))\n","  #쿼리를 도는것\n","  for q in range(num_query):\n","    dist = np.zeros(num_gallery)\n","    #갤러리를 도는 것\n","    for g in range(num_gallery):\n","      #벡터와 벡터 사이의 차\n","      #의 제곱\n","      #dist의 길이는 gallary만큼의 길이를 가짐 why? 한 쿼리에 대해 모든 갤러리 샘플 비교해 디스턴스 구하기 떄문\n","      dist[g] = np.sqrt(np.sum((query[q,:] - gallery[g,:]) ** 2, axis=0))\n","    #q번째 쿼리 샘플에 대해 가장 가까운 갤러리 인덱스 저장\n","\n","    if to == 1:\n","      nn_idx[q] = np.argmin(dist)\n","    else:\n","      nn_idx2[q] = dist.argsort()[:5]\n","  \n","  return nn_idx if to == 1 else nn_idx2"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"s2sHU4JGsQXf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621836123836,"user_tz":-540,"elapsed":2402,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"89f0911b-95ce-49b8-8353-6a434a8e49a6"},"source":["# print(test_query_feature.shape)\n","# test_query_feature == test_query_data.reshape(500,512)\n","# print(test_query_data.shape)\n","# print(test_gallery_feature.shape)\n","#노말리제이션으로 성능 올리기 가능\n","#nn_idx_deep=getNearestNeibor(test_query_feature,test_gallery_feature)\n","# nn_idx_deep=getNearestNeibor(normalized(test_query_feature,axis=1,order=2),normalized(test_gallery_feature,axis=1,order=2))\n","# print(np.sum(test_query_label == test_gallery_label[np.int64(nn_idx_deep)]))\n","# print(np.sum(test_query_label == test_gallery_label[np.int64(nn_idx)]))\n","\n","v1_test_query_feature == v1_test_query_data.reshape(1000,512)\n","nn_idx_deep_real=getNearestNeibor(normalized(v1_test_query_feature,axis=1,order=2),normalized(test_gallery_feature,axis=1,order=2), to=1)\n","nn_idx_deep_real_real=getNearestNeibor(normalized(v1_test_query_feature,axis=1,order=2),normalized(test_gallery_feature,axis=1,order=2), to=5)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["<class 'int'>\n","<class 'int'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e-q7GVXPYU4Q","executionInfo":{"status":"ok","timestamp":1621836123838,"user_tz":-540,"elapsed":7,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["import pickle\n","\n","with open('team9.pickle', 'wb') as f:\n","    pickle.dump(nn_idx_deep_real_real, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sFp_TGwGKmH","executionInfo":{"status":"ok","timestamp":1621836123839,"user_tz":-540,"elapsed":7,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"26655905-cf0d-4275-829d-0d43faa4d895"},"source":["#정확도는 ?/1000이 됨\n","pred_test_gallery_label_txt = list_data = [str(int(a)).strip('\\n\\r') for a in test_gallery_label[np.int64(nn_idx_deep_real)]]\n","print(len(pred_test_gallery_label_txt))\n","#print(train_data[0:10])"],"execution_count":24,"outputs":[{"output_type":"stream","text":["1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FqFwt296tIHk","executionInfo":{"status":"ok","timestamp":1621836123839,"user_tz":-540,"elapsed":4,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["# from sklearn.neighbors import NearestNeighbors\n","# nbrs=NearestNeighbors(n_neighbors=1).fit(normalized(test_gallery_feature,axis=1,order=2))\n","# #order는 l1, l2를 정함\n","# #order가 1이면 l1\n","# #하지만 큰 차이가 나지는 않음\n","# distance, indices=nbrs.kneighbors(normalized(test_query_feature,axis=1,order=2))\n","# print(np.sum(test_query_label == test_gallery_label[np.int64(indices.squeeze())]))"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzg73mnQ4uIh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621836142888,"user_tz":-540,"elapsed":19053,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"3cdbe109-0a11-4d38-a0a1-2e0bc7a515e3"},"source":["pip install pycryptodomex --no-binary :all:"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Collecting pycryptodomex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/e2/a0f9f5452a59bafaa3420585f22b58a8566c4717a88c139af2276bb5695d/pycryptodomex-3.10.1.tar.gz (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 5.5MB/s \n","\u001b[?25hSkipping wheel build for pycryptodomex, due to binaries being disabled for it.\n","Installing collected packages: pycryptodomex\n","    Running setup.py install for pycryptodomex ... \u001b[?25l\u001b[?25hdone\n","Successfully installed pycryptodomex-3.10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-SQAU6kK4g1b","executionInfo":{"status":"ok","timestamp":1621836144170,"user_tz":-540,"elapsed":1286,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["import json\n","from base64 import b64encode\n","from Cryptodome.Cipher import AES\n","from Cryptodome.Util.Padding import pad\n","\n","def read_txt(fileName):\n","    with open(fileName, 'rt') as f:\n","        list_data = [a.strip('\\n\\r') for a in f.readlines()]\n","    return list_data\n","\n","def write_json(fileName, data):\n","    with open(fileName, 'w', encoding='utf-8') as f:\n","        json.dump(data, f, ensure_ascii=False, indent=4)\n","\n","def load_key(key_path):\n","    with open(key_path, \"rb\") as f:\n","        key = f.read()\n","    return key\n","\n","def encrypt_data(key_path, ans_list, encrypt_store_path='ans.json'):\n","    key = load_key(key_path)\n","    data = \" \".join([str(i) for i in ans_list])\n","    encode_data = data.encode()\n","    cipher = AES.new(key, AES.MODE_CBC)\n","    ct_bytes = cipher.encrypt(pad(encode_data, AES.block_size))\n","    iv = b64encode(cipher.iv).decode('utf-8')\n","    ct = b64encode(ct_bytes).decode('utf-8')\n","    write_json(encrypt_store_path, {'iv':iv, 'ciphertext':ct})\n","\n","if __name__==\"__main__\":\n","    # 1.이메일을 통해서 전달 받은 키 파일의 경로 입력\n","    key_path = \"/content/drive/MyDrive/인공지능 수업/team9.pem\"\n","    # 2. 예측한 결과를 텍스트 파일로 저장했을 경우 리스트로 다시 불러오기\n","    # 본인이 원하는 방식으로 리스트 형태로 예측 값을 불러오기만 하면 됨(순서를 지킬것)\n","    #raw_ans_path = \"ans.txt\"\n","    #ans = read_txt(raw_ans_path)\n","    ans = pred_test_gallery_label_txt\n","    # 3. 암호화된 파일을 저장할 위치\n","    encrypt_ans_path = \"/content/drive/MyDrive/인공지능 수업/ai_answer.json\"\n","    # 4. 암호화!(pycrytodome 설치)\n","    encrypt_data(key_path, ans, encrypt_ans_path)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhL6RaN3AElN"},"source":["# 새 섹션"]}]}